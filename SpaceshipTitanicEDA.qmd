---
title: Spaceship Titanic Investigation
author: 
  - name: Adam Gilbert
    email: a.gilbert1@snhu.edu
    affiliations: 
      - name: Southern New Hampshire University
format: html
date: 9/15/2023
date-modified: today
date-format: long
theme: flatly
toc: true
code-fold: true
---

```{r setup}
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(patchwork)
library(kableExtra)
library(ggridges)
library(marginaleffects)

data <- read_csv("https://raw.githubusercontent.com/agmath/agmath.github.io/master/data/classification/spaceship_titanic.csv")

names(data) <- janitor::make_clean_names(names(data))

data <- data %>%
  mutate(
    transported = factor(transported)
  )

data_splits <- initial_split(data, prop = 0.8, strata = transported)

train <- training(data_splits)
test <- testing(data_splits)
```

## Statement of Purpose

Like its namesake vessel, the *Spaceship Titanic* encountered tragedy when several of its passengers were warped to an alternate dimension during flight! This analysis is a post-mortem on the flight and passenger list so that we may better understand who is at risk for interdimensional transport during spaceflight and can take future precautionary measures.

## Executive Summary

:::{.callout-note}
Intentionally left blank, for now. This is the last section to be written.
:::

## Introduction

The year is 2063. We’ve come a long way from the early 2020’s, where billionaire tech entrepreneurs launched tiny rockets, holding a handful of celebrities or wealthy elites, into near-Earth orbit for an exorbitant pricetag. The future is now…well, was last week… Things are much more uncertain now. We were so excited with the launch of the Spaceship Titanic. It was supposed to be the beginning of a new era – affordable, long-range space travel for everyone. In hindsight, perhaps naming the thing the Titanic was a poor decision – too tempting for fate and karma.

In any case, space travel is an important venture for humanity at this point in our history as a species. Demand is high, even with last week’s disaster. We have a vested interest in safe and reliable travel through the cosmos and need to better understand what happened to the travelers who’ve disappeared and why it happened to them and not other passengers. Demand for space travel was expected to reach 86 million travelers next year – we can’t continue if we only expect 43 million passengers to arrive at their intended destination.

## Exploratory Data Analysis

The original data set on the passengers contained `r data %>% nrow()` passengers and `r data %>% ncol()` features (variables). We can see the first-few passengers'-worth of data printed out below.

```{r}
data %>%
  head() %>%
  kable()
```

That data was split into a collection of `r train %>% nrow()` training observations and `r test %>% nrow()` test observations for validating our final model's performance. Care was taken to ensure that the passengers who were *transported* to an alternate dimension are proportionally represented across the *training* and *test* sets.

### Univariate Analyses

Since our goal is to understand who was transported to an alternate dimension during flight and perhaps gain some insight as to why they were transported, we’ll start by understanding the transported variable and the distributions of the other variables available to us.

```{r}
train %>%
  ggplot() + 
  geom_bar(aes(x = transported)) + 
  labs(title = "Distribution of Transported Passengers",
       x = "Interdimensional Transport Status",
       y = "Count")
```

```{r}
pct_transported <- train %>%
  count(transported) %>%
  ungroup() %>%
  mutate(pct = 100*n/sum(n)) %>%
  filter(transported == "yes") %>%
  pull(pct)
```

The percentage of passengers transported in the training set is about `r round(pct_transported, 2)`%. Let’s look at the distributions of the other categorical variables in the data set.

```{r}
p1 <- train %>%
  ggplot() + 
  geom_bar(aes(x = home_planet)) + 
  labs(
    title = "Boarding Planet",
    x = "",
    y = "Count"
  ) + 
  coord_flip()

p2 <- train %>%
  ggplot() + 
  geom_bar(aes(x = cryo_sleep)) +
  labs(
    title = "CryoSleep Selection",
    y = "Count",
    x = ""
  )

p3 <- train %>%
  ggplot() + 
  geom_bar(aes(x = destination)) + 
  labs(
    title = "Destination Planet",
    x = "",
    y = "Count"
  ) + 
  coord_flip()

p4 <- train %>%
  ggplot() + 
  geom_bar(aes(x = vip)) + 
  labs(
    title = "VIP Status",
    x = "",
    y = "Count"
  )

(p1 + p2) / (p3 + p4)
```

From the top-left plot, we see that the majority of passengers board on Earth, while fewer passengers board on Europa and Mars. Some passengers have no boarding planet information (`NA`) – perhaps these passengers are crew members. In the top-right plot, we see that nearly 2/3 of passengers choose the Cryo Sleep option, while around 1/3 do not. Again, some passengers have missing data here. The distribution of destination planet is shown in the lower-right, and tells us that the most popular destination (by a large margin) is TRAPPIST-1e. The only other two destination planets are PSO J318.5-22 and 55 Cancri e. As in the previous plots, some passengers do not have an identified destination. Finally, the proportion of passengers with VIP status is about 2.23.

In each of the plots, we identified several passengers with missing values. There are 0 passengers missing information for all four of these variables. This means that our earlier conjecture about those passengers being crew is unlikely.

Let’s continue on to view the distributions of the numerical predictors available to us. We’ll start with the distribution of passenger ages.

```{r}
#| message: false
#| warning: false

train %>%
  ggplot() + 
  geom_histogram(aes(x = age), color = "black",
                 fill = "purple") + 
  labs(
    title = "Passenger Ages",
    x = "Age (Years)",
    y = ""
  )
```

The plot above shows a [near] 0-inflated distribution. That is, there is an inflated number of observations near 0, given the shape of the rest of the distribution. Ages are right-skewed, with a median passenger age of 27 years old. Next we’ll look at the distribution of room service charges.

```{r}
#| message: false
#| warning: false

p1 <- train %>%
  ggplot() + 
  geom_density(aes(x = room_service),
               fill = "purple",
               color = "black") + 
  geom_boxplot(aes(x = room_service, y = -0.005),
               fill = "purple",
               width = 0.002) + 
  labs(
    title = "Room Service Money Spent",
    x = "Expenditure",
    y = ""
    )

p2 <- train %>%
  ggplot() + 
  geom_density(aes(x = room_service),
               fill = "purple",
               color = "black") + 
  geom_boxplot(aes(x = room_service, y = -0.05),
               fill = "purple",
               width = 0.02) + 
  labs(
    title = "Room Service Money Spent",
    x = "Expenditure",
    y = ""
    ) + 
  scale_x_log10()

p1 + p2
```

Both of the plots above show the distribution of room service expenditures. From the plot on the left, we can see that the distribution is very strongly right-skewed. The majority of passengers spent very little on room service, but there were some passengers who ran up extremely large tabs! The plot on the right shows the same variable but on a logarithmic scale. This particular transformation ignores passengers who did not spend any money on room service. From this plot, we actually see that the median room service expenditure among passengers who utilized room service is quite high – it is about `r median(train$room_service)`. We’ll continue our exploration of the available numerical features below, by working with the expenditures at the food court, shopping mall, spa, and VR deck. All of these are right skewed so we’ll just show the distributions on a logarithmic scale.

```{r}
#| warning: false
#| messae: false

p_food <- train %>%
  ggplot() + 
  geom_density(aes(x = food_court), fill = "purple") + 
  geom_boxplot(aes(x = food_court, y = -0.075), 
               fill = "purple", width = 0.05) +
  scale_x_log10() +
  labs(title = "Food Court Expenditures",
       x = "Money Spent",
       y = "")

p_shop <- train %>%
  ggplot() + 
  geom_density(aes(x = shopping_mall), fill = "purple") + 
  geom_boxplot(aes(x = shopping_mall, y = -0.075), 
               fill = "purple", width = 0.05) +
  scale_x_log10() +
  labs(title = "Shopping Mall Expenditures",
       x = "Money Spent",
       y = "")

p_spa <- train %>%
  ggplot() + 
  geom_density(aes(x = spa), fill = "purple") + 
  geom_boxplot(aes(x = spa, y = -0.075), 
               fill = "purple", width = 0.05) +
  scale_x_log10() +
  labs(title = "Spa Expenditures",
       x = "Money Spent",
       y = "")

p_vr <- train %>%
  ggplot() + 
  geom_density(aes(x = vr_deck), fill = "purple") + 
  geom_boxplot(aes(x = vr_deck, y = -0.075), 
               fill = "purple", width = 0.05) +
  scale_x_log10() +
  labs(title = "VR Deck Expenditures",
       x = "Money Spent",
       y = "")

(p_food + p_shop) / (p_spa + p_vr)
```

The distributions of these variables are all quite similar to one another. The distributions are skewed and 0-inflated. The distributions remain left-skewed even when plotted on a logarithmic scale and the observations at 0 are removed. The mean, median, standard deviation, and interquartile range for each expenditure venue are reported below without the removal of those zero observations.

```{r}
train %>%
  pivot_longer(cols = c("room_service", "food_court", "shopping_mall", "spa", "vr_deck"), 
               names_to = "Venue",
               values_to = "Expenditure") %>%
  select(Venue, Expenditure) %>%
  group_by(Venue) %>%
  summarize(mean_expenditure = mean(Expenditure, na.rm = TRUE),
            median_expenditure = median(Expenditure, na.rm = TRUE),
            sd_expenditure = sd(Expenditure, na.rm = TRUE),
            iqr_expenditure = IQR(Expenditure, na.rm = TRUE)) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```

The same metrics are reported below after removal of the zero expenditure values. That is, the summary metrics reported below consider only passengers who utilized the corresponding services. These values will align with measures indicated from the log-scale plots above.

```{r}
train %>%
  pivot_longer(cols = c("room_service", "food_court", "shopping_mall", "spa", "vr_deck"), 
               names_to = "Venue",
               values_to = "Expenditure") %>%
  select(Venue, Expenditure) %>%
  filter(Expenditure > 0) %>%
  group_by(Venue) %>%
  summarize(mean_expenditure = mean(Expenditure, na.rm = TRUE),
            median_expenditure = median(Expenditure, na.rm = TRUE),
            sd_expenditure = sd(Expenditure, na.rm = TRUE),
            iqr_expenditure = IQR(Expenditure, na.rm = TRUE)) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```

Below is a visual representation of the distributions for these expenditure variables with the observations at 0 expenditure removed.

```{r}
#| message: false
#| warning: false

train %>%
  pivot_longer(cols = c("room_service", "food_court", "shopping_mall", "spa", "vr_deck"), 
               names_to = "Venue",
               values_to = "Expenditure") %>%
  select(Venue, Expenditure) %>%
  filter(Expenditure > 0) %>%
  ggplot() + 
  geom_density_ridges(aes(x = Expenditure, y = Venue, fill = Venue),
                      scale = 0.5) + 
  geom_boxplot(aes(x = Expenditure, fill = Venue, y = Venue), width = 0.05) + 
  scale_x_log10(labels = scales::dollar_format()) + 
  labs(title = "Expenditure Distributions",
       x = "Money Spent",
       y = "") + 
  theme(legend.position = "None")
```

### Multivariate Analyses

Now that we understand the individual distributions of the variables, its time to look at how these predictors are associated with out response variable (transported). We’ll begin by looking for associations between transported and the categorical variables.

```{r}
p_home <- train %>%
  ggplot() + 
  geom_bar(aes(y = home_planet,
               fill = transported),
           position = "dodge",
           show.legend = FALSE) + 
  labs(title = "Home Planet and Transport",
       x = "Count",
       y = "")

p_cryo <- train %>%
  ggplot() + 
  geom_bar(aes(x = cryo_sleep,
               fill = transported),
           position = "dodge") + 
  labs(title = "Cryo Sleep and Transport",
       x = "",
       y = "Count")

p_destination <- train %>%
  ggplot() + 
  geom_bar(aes(y = destination,
               fill = transported),
           position = "dodge",
           show.legend = FALSE) + 
  labs(title = "Destination and Transport",
       x = "Count",
       y = "")

p_vip <- train %>%
  ggplot() + 
  geom_bar(aes(x = vip,
               fill = transported),
           position = "fill",
           show.legend = FALSE) + 
  labs(title = "VIP Status and Transport",
       x = "",
       y = "Proportion")

(p_home + p_cryo) / (p_destination + p_vip)
```

From the four plots above, we have the following takeaways. First, the plot on the left shows the passengers from Europa were much more likely to be transported than passengers from Mars or Earth. Passengers from Earth had a less than 50% transport rate while passengers from Mars had a slightly larger than 50% transport rate. Passengers in Cryo Sleep had an extremely elevated likelihood of transport than those who did not take advantage of Cryo Sleep. There were slight differences in transport rates by destination and by VIP status, but the choice to undergo Cryo Sleep seems to have been the largest influence over whether passengers were transported or not.

Now we’ll consider how the numerical features may be associated with the `transported` status of passengers.

```{r}
#| message: false
#| warning: false

train %>%
  pivot_longer(cols = c("room_service", "food_court", "shopping_mall", "spa", "vr_deck"), 
               names_to = "Venue",
               values_to = "Expenditure") %>%
  mutate(transported = ifelse(transported == "yes", "transported", "not")) %>%
  ggplot() + 
  geom_boxplot(aes(x = Expenditure,
                   y = transported,
                   fill = Venue),
               show.legend = FALSE) +
  scale_x_log10() +
  facet_wrap(~Venue) + 
  labs(title = "",
       x = "Expenditure",
       y = "")
```

In the group of plots appearing above, we see that higher food court and shopping mall expenditures were associate with those passengers who were transported than those who were not. Those individuals not being transported had higher room service, spa, and VR deck expenditures on average than those who were not transported.

As a result of this exploratory analysis, we’ve identified several important insights as we proceed to the model construction phase of this analysis. Firstly, about half of passengers were transported to an alternate dimension while the other half were transported safely. All of the numerical features are very heavily right-skewed aside from age. The variable most strongly associated with whether or not a passenger was transported may be the choice to Cryo Sleep during the flight. Other variables showed associations as well, but were less pronounced.

## Model Construction and Assessment

:::{.callout-important}
## Statistical Learning versus Machine Learning

There are two competing objectives that we can have in model construction. 

+ **Statistical Learning:** We build models with the intent of discovering and interpreting associations between our available predictors and the response.
  
  + Fits models and then uses $p$-values to identify significant predictors, confidence intervals for coefficients, etc.

+ **Machine Learning:** We build models with the intent of making predictions as accurately as possible. 

  + Uses methods like *cross-validation* for hyperparameter tuning and performance estimation

You do not necessarily need to choose just one approach or the other. Perhaps the problems/challenges you’ve set out to solve would benefit from both descriptive and predictive models. In this case, just make clear to the reader when you are switching between objectives.
:::

### Statistical Learning

#### Logistic Regression

```{r}
log_reg_spec <- logistic_reg() |>
  set_engine("glm") |>
  set_mode("classification")

log_reg_rec <- recipe(transported ~ ., data = train) |>
  step_rm(passenger_id) |>
  step_rm(name) |>
  step_rm(cabin) |>
  step_rm(vip) |>
  step_impute_median(all_numeric_predictors()) |>
  step_impute_mode(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors())

log_reg_wf <- workflow() |>
  add_model(log_reg_spec) |>
  add_recipe(log_reg_rec)

log_reg_fit <- log_reg_wf |>
  fit(train)

log_reg_fit |>
  extract_fit_engine() |>
  tidy() |>
  mutate(
    odds_ratio = exp(estimate),
    odds_lower = exp(estimate - 2*std.error),
    odds_upper = exp(estimate + 2*std.error)
  )
```

#### Marignal Effects

```{r}
new_data <- crossing(
  passenger_id = NA,
  home_planet = "Europa",
  cryo_sleep = "no",
  cabin = NA,
  destination = "TRAPPIST-1e",
  age = mean(train$age, na.rm = TRUE),
  vip = "no",
  room_service = mean(train$room_service, na.rm = TRUE),
  food_court = mean(train$food_court, na.rm = TRUE),
  shopping_mall = mean(train$shopping_mall, na.rm = TRUE),
  spa = mean(train$spa, na.rm = TRUE),
  vr_deck = seq(min(train$vr_deck, na.rm = TRUE),
            max(train$vr_deck, na.rm = TRUE),
            by = 1),
  name = NA
)

new_data_baked <- log_reg_rec %>%
  prep() %>%
  bake(new_data)

mfx <- slopes(log_reg_fit %>% extract_fit_parsnip(),
              newdata = new_data_baked,
              variables = "age",
              type = "prob") %>%
  tibble()

mfx %>%
  select(term, vr_deck, estimate, conf.low, conf.high, std.error) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))

mfx %>%
  arrange(age)

mfx %>%
  filter(group == "yes") %>%
  select(vr_deck, estimate, conf.low, conf.high) %>%
  ggplot() +
  geom_line(aes(x = vr_deck, y = estimate), color = "purple", lty = "dashed", lwd = 1.5) +
  geom_ribbon(aes(x = vr_deck, ymin = conf.low, ymax = conf.high),
              fill = "grey", alpha = 0.5) +
  labs(x = "VR Deck",
       y = "Marginal Effect",
       title = "Marginal Effects of Unit Increase in Weight")
```


### Machine Learning

#### Logistic Regression

## Model Interpretation and Inference

## Conclusion

## References



